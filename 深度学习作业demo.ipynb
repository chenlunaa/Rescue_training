{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2f3593",
   "metadata": {},
   "source": [
    "这个是深度学习课后的小作业，主要是给大家再熟悉一下模型训练的流程，让大家体验一下。我们这里的任务是对10个类别的“时装”图像进行分类，使用[FashionMNIST数据集](https://github.com/zalandoresearch/fashion-mnist/tree/master/data/fashion )。 \n",
    "FashionMNIST数据集中包含已经预先划分好的训练集和测试集，其中训练集共60,000张图像，测试集共10,000张图像。每张图像均为单通道黑白图像，大小为28\\*28pixel，分属10个类别。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd159071",
   "metadata": {},
   "source": [
    "**首先导入必要的包** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "927d4420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e9da3",
   "metadata": {},
   "source": [
    "**配置训练环境和超参数** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a2392e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 配置GPU，这里有两种方式\n",
    "## 方案一：使用os.environ\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "# 方案二：使用“device”，后续对要使用GPU的变量用.to(device)即可\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "## 配置其他超参数，如batch_size, num_workers, learning rate, 以及总的epochs\n",
    "batch_size = 256\n",
    "num_workers = 0   # 对于Windows用户，这里应设置为0，否则会出现多线程错误\n",
    "lr = 1e-4\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e06dc2",
   "metadata": {},
   "source": [
    "**数据读入和加载**  \n",
    "这里同时展示两种方式:  \n",
    "- 下载并使用PyTorch提供的内置数据集  \n",
    "- 从网站下载以csv格式存储的数据，读入并转成预期的格式    \n",
    "第一种数据读入方式只适用于常见的数据集，如MNIST，CIFAR10等，PyTorch官方提供了数据下载。这种方式往往适用于快速测试方法（比如测试下某个idea在MNIST数据集上是否有效）  \n",
    "第二种数据读入方式需要自己构建Dataset，这对于PyTorch应用于自己的工作中十分重要  \n",
    "  \n",
    "同时，还需要对数据进行必要的变换，比如说需要将图片统一为一致的大小，以便后续能够输入网络训练；需要将数据格式转为Tensor类，等等。\n",
    "  \n",
    "**我们实际比赛的时候数据加载和处理是一个非常关键的步骤。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3947bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先设置数据变换\n",
    "from torchvision import transforms\n",
    "\n",
    "image_size = 28\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  \n",
    "     # 这一步取决于后续的数据读取方式，如果使用内置数据集读取方式则不需要\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a86217a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 读取方式一：使用torchvision自带数据集，下载可能需要一段时间\n",
    "# from torchvision import datasets\n",
    "\n",
    "# train_data = datasets.FashionMNIST(root='./', train=True, download=True, transform=data_transform)\n",
    "# test_data = datasets.FashionMNIST(root='./', train=False, download=True, transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7e5a1003",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取方式二：读入csv格式的数据，自行构建Dataset类\n",
    "# csv数据下载链接：https://www.kaggle.com/zalando-research/fashionmnist\n",
    "class FMDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.images = df.iloc[:,1:].values.astype(np.uint8)\n",
    "        self.labels = df.iloc[:, 0].values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx].reshape(28,28,1)\n",
    "        label = int(self.labels[idx])\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = torch.tensor(image/255., dtype=torch.float)\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "        return image, label\n",
    "\n",
    "train_df = pd.read_csv(\"./FashionMNIST/fashion-mnist_train.csv\")\n",
    "test_df = pd.read_csv(\"./FashionMNIST/fashion-mnist_test.csv\")\n",
    "train_data = FMDataset(train_df, data_transform)\n",
    "test_data = FMDataset(test_df, data_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb0b8b1",
   "metadata": {},
   "source": [
    "在构建训练和测试数据集完成后，需要定义DataLoader类，以便在训练和测试时加载数据  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a1d8f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0069a065",
   "metadata": {},
   "source": [
    "读入后，我们可以做一些数据可视化操作，主要是验证我们读入的数据是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "784617fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1, 28, 28]) torch.Size([256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1e372bf6490>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = next(iter(train_loader))\n",
    "print(image.shape, label.shape)\n",
    "plt.imshow(image[0][0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded3b04",
   "metadata": {},
   "source": [
    "**模型设计**  \n",
    "由于任务较为简单，这里我们手搭一个CNN，而不考虑当下各种模型的复杂结构，模型构建完成后，将模型放到GPU上用于训练。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8817a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(32, 64, 5),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(64*4*4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 64*4*4)\n",
    "        x = self.fc(x)\n",
    "        # x = nn.functional.normalize(x)\n",
    "        return x\n",
    "\n",
    "model = Net()\n",
    "model = model.cuda()\n",
    "# model = nn.DataParallel(model).cuda()   # 多卡训练时的写法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb67c0f8",
   "metadata": {},
   "source": [
    "**设定损失函数**  \n",
    "使用torch.nn模块自带的CrossEntropy损失  \n",
    "PyTorch会自动把整数型的label转为one-hot型，用于计算CE loss  \n",
    "这里需要确保label是从0开始的，同时模型不加softmax层（使用logits计算）,这也说明了PyTorch训练中各个部分不是独立的，需要通盘考虑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "680b8ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.CrossEntropyLoss(weight=[1,1,1,1,3,1,1,1,1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eda2704",
   "metadata": {},
   "source": [
    "**设定优化器**  \n",
    "这里我们使用Adam优化器 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "0315515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f395c64",
   "metadata": {},
   "source": [
    "**训练流程（自己写）**\n",
    "调⽤model.train()\n",
    "(1) 从train_dataloader中加载数据\n",
    "(2) 计算损失函数\n",
    "(3) 反向传播，优化器优化\n",
    "(4) print展⽰输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6cb6bd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    total_train_step = 0\n",
    "    for i in range(epoch):\n",
    "        print(\"-----------------第{}轮训练开始--------------------\".format(i+1))\n",
    "        model.train()\n",
    "        for data in train_loader:\n",
    "            imgs, targets = data\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_step = total_train_step + 1\n",
    "            if total_train_step % 100 == 0:\n",
    "                print(\"训练次数: {}, Loss: {}\".format(total_train_step, loss.item()))\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bdb39f",
   "metadata": {},
   "source": [
    "**验证流程（自己写）**\n",
    "关注两者的主要区别：  \n",
    "- 模型状态设置  \n",
    "- 是否需要初始化优化器\n",
    "- 是否需要将loss传回到网络\n",
    "- 是否需要每步更新optimizer  \n",
    "  \n",
    "此外，对于测试或验证过程，可以计算分类准确率，要求把结果print出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a44cb93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    model.eval()\n",
    "    total_test_step = 0\n",
    "    total_test_loss = 0\n",
    "    total_acc_sum = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            imgs, targets = data\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_test_loss = total_test_loss + loss.item()\n",
    "            acc_sum = (outputs.argmax(1) == targets).sum().item()\n",
    "            total_acc_sum = total_acc_sum + acc_sum\n",
    "\n",
    "    print (\"整体测试数据集上的Loss: {}\".format(total_test_loss))\n",
    "    print (\"整体测试数据集上的acc: {}\".format(total_acc_sum / (test_data.__len__() )))\n",
    "    total_test_step = total_test_step + 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f524f896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.5960187911987305\n",
      "训练次数: 200, Loss: 0.5186540484428406\n",
      "整体测试数据集上的Loss: 18.256870299577713\n",
      "整体测试数据集上的acc: 0.8453\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.3961816430091858\n",
      "训练次数: 200, Loss: 0.4749125838279724\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.43052324652671814\n",
      "训练次数: 400, Loss: 0.4046628773212433\n",
      "整体测试数据集上的Loss: 12.458986729383469\n",
      "整体测试数据集上的acc: 0.8877\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.3284989297389984\n",
      "训练次数: 200, Loss: 0.32395562529563904\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.3213372528553009\n",
      "训练次数: 400, Loss: 0.3293324410915375\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.3080592453479767\n",
      "训练次数: 600, Loss: 0.23699945211410522\n",
      "训练次数: 700, Loss: 0.3737263083457947\n",
      "整体测试数据集上的Loss: 10.592954531311989\n",
      "整体测试数据集上的acc: 0.9024\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.3145139515399933\n",
      "训练次数: 200, Loss: 0.3187974691390991\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.24259111285209656\n",
      "训练次数: 400, Loss: 0.22074346244335175\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.3472801148891449\n",
      "训练次数: 600, Loss: 0.28635692596435547\n",
      "训练次数: 700, Loss: 0.23866398632526398\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.27673715353012085\n",
      "训练次数: 900, Loss: 0.2561018168926239\n",
      "整体测试数据集上的Loss: 9.2308194860816\n",
      "整体测试数据集上的acc: 0.9119\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.22568561136722565\n",
      "训练次数: 200, Loss: 0.2488948106765747\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.23216553032398224\n",
      "训练次数: 400, Loss: 0.21912993490695953\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.1752329021692276\n",
      "训练次数: 600, Loss: 0.23031538724899292\n",
      "训练次数: 700, Loss: 0.2246505171060562\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.2187740057706833\n",
      "训练次数: 900, Loss: 0.21319660544395447\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.2029138058423996\n",
      "训练次数: 1100, Loss: 0.19206826388835907\n",
      "整体测试数据集上的Loss: 8.88787342607975\n",
      "整体测试数据集上的acc: 0.9163\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.2112218141555786\n",
      "训练次数: 200, Loss: 0.19704250991344452\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.19623062014579773\n",
      "训练次数: 400, Loss: 0.23375505208969116\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.22379784286022186\n",
      "训练次数: 600, Loss: 0.15315552055835724\n",
      "训练次数: 700, Loss: 0.17360824346542358\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.18594512343406677\n",
      "训练次数: 900, Loss: 0.22484515607357025\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.22300468385219574\n",
      "训练次数: 1100, Loss: 0.1854921132326126\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.20588381588459015\n",
      "训练次数: 1300, Loss: 0.2145092636346817\n",
      "训练次数: 1400, Loss: 0.18068891763687134\n",
      "整体测试数据集上的Loss: 8.153928443789482\n",
      "整体测试数据集上的acc: 0.9239\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.15733285248279572\n",
      "训练次数: 200, Loss: 0.1646914929151535\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.15600605309009552\n",
      "训练次数: 400, Loss: 0.16173896193504333\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.20046404004096985\n",
      "训练次数: 600, Loss: 0.1551234871149063\n",
      "训练次数: 700, Loss: 0.13854245841503143\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.1929762363433838\n",
      "训练次数: 900, Loss: 0.16809143126010895\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.17425653338432312\n",
      "训练次数: 1100, Loss: 0.0980193167924881\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.17082549631595612\n",
      "训练次数: 1300, Loss: 0.0992722138762474\n",
      "训练次数: 1400, Loss: 0.15228275954723358\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.11352798342704773\n",
      "训练次数: 1600, Loss: 0.12989512085914612\n",
      "整体测试数据集上的Loss: 8.321203246712685\n",
      "整体测试数据集上的acc: 0.9221\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.13351616263389587\n",
      "训练次数: 200, Loss: 0.11874181777238846\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.11187226325273514\n",
      "训练次数: 400, Loss: 0.18308089673519135\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.07683596760034561\n",
      "训练次数: 600, Loss: 0.14415308833122253\n",
      "训练次数: 700, Loss: 0.13391385972499847\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.116597481071949\n",
      "训练次数: 900, Loss: 0.1565384715795517\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.10532210767269135\n",
      "训练次数: 1100, Loss: 0.1940811276435852\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.10395143181085587\n",
      "训练次数: 1300, Loss: 0.2011493742465973\n",
      "训练次数: 1400, Loss: 0.11659593135118484\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.11135246604681015\n",
      "训练次数: 1600, Loss: 0.12208966910839081\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.11489503085613251\n",
      "训练次数: 1800, Loss: 0.14749372005462646\n",
      "整体测试数据集上的Loss: 8.976982839405537\n",
      "整体测试数据集上的acc: 0.9272\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.12257339805364609\n",
      "训练次数: 200, Loss: 0.0798848494887352\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.12389958649873734\n",
      "训练次数: 400, Loss: 0.06056288257241249\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.07096989452838898\n",
      "训练次数: 600, Loss: 0.1404062956571579\n",
      "训练次数: 700, Loss: 0.13336624205112457\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.13311238586902618\n",
      "训练次数: 900, Loss: 0.10901474207639694\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.09652161598205566\n",
      "训练次数: 1100, Loss: 0.09889527410268784\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.09293515980243683\n",
      "训练次数: 1300, Loss: 0.10936184227466583\n",
      "训练次数: 1400, Loss: 0.09533310681581497\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.0885794535279274\n",
      "训练次数: 1600, Loss: 0.07848982512950897\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.09291655570268631\n",
      "训练次数: 1800, Loss: 0.15056005120277405\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.12490957975387573\n",
      "训练次数: 2000, Loss: 0.11383108794689178\n",
      "训练次数: 2100, Loss: 0.06850564479827881\n",
      "整体测试数据集上的Loss: 9.168181836605072\n",
      "整体测试数据集上的acc: 0.9279\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.12717826664447784\n",
      "训练次数: 200, Loss: 0.08083651214838028\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.16707217693328857\n",
      "训练次数: 400, Loss: 0.11606988310813904\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.13027627766132355\n",
      "训练次数: 600, Loss: 0.06971405446529388\n",
      "训练次数: 700, Loss: 0.13009251654148102\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.10311683267354965\n",
      "训练次数: 900, Loss: 0.0985700711607933\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.10928676277399063\n",
      "训练次数: 1100, Loss: 0.08988134562969208\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.09180430322885513\n",
      "训练次数: 1300, Loss: 0.09533263742923737\n",
      "训练次数: 1400, Loss: 0.07663482427597046\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.10204706341028214\n",
      "训练次数: 1600, Loss: 0.0784677192568779\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.05949975177645683\n",
      "训练次数: 1800, Loss: 0.08326631784439087\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.07049836218357086\n",
      "训练次数: 2000, Loss: 0.07854466140270233\n",
      "训练次数: 2100, Loss: 0.07217273861169815\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.0484989657998085\n",
      "训练次数: 2300, Loss: 0.06800265610218048\n",
      "整体测试数据集上的Loss: 9.494797706604004\n",
      "整体测试数据集上的acc: 0.9295\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.06257005035877228\n",
      "训练次数: 200, Loss: 0.12185940891504288\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.11428898572921753\n",
      "训练次数: 400, Loss: 0.06740287691354752\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.05912885442376137\n",
      "训练次数: 600, Loss: 0.10874777287244797\n",
      "训练次数: 700, Loss: 0.08590637147426605\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.04731166362762451\n",
      "训练次数: 900, Loss: 0.09950168430805206\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.08532964438199997\n",
      "训练次数: 1100, Loss: 0.09808512777090073\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.07581935077905655\n",
      "训练次数: 1300, Loss: 0.08006741106510162\n",
      "训练次数: 1400, Loss: 0.10744254291057587\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.0650433674454689\n",
      "训练次数: 1600, Loss: 0.08913971483707428\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.0544649101793766\n",
      "训练次数: 1800, Loss: 0.05684691295027733\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.04503442719578743\n",
      "训练次数: 2000, Loss: 0.11998803913593292\n",
      "训练次数: 2100, Loss: 0.09807441383600235\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.08975347131490707\n",
      "训练次数: 2300, Loss: 0.0527665801346302\n",
      "-----------------第11轮训练开始--------------------\n",
      "训练次数: 2400, Loss: 0.08364758640527725\n",
      "训练次数: 2500, Loss: 0.12647907435894012\n",
      "整体测试数据集上的Loss: 10.317622929811478\n",
      "整体测试数据集上的acc: 0.9298\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.04007236286997795\n",
      "训练次数: 200, Loss: 0.11101987212896347\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.03167390450835228\n",
      "训练次数: 400, Loss: 0.0901276022195816\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.04309951141476631\n",
      "训练次数: 600, Loss: 0.08994908630847931\n",
      "训练次数: 700, Loss: 0.03810524195432663\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.03905661776661873\n",
      "训练次数: 900, Loss: 0.11564856767654419\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.055398404598236084\n",
      "训练次数: 1100, Loss: 0.060788821429014206\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.0964139848947525\n",
      "训练次数: 1300, Loss: 0.08227424323558807\n",
      "训练次数: 1400, Loss: 0.11814302951097488\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.04599522426724434\n",
      "训练次数: 1600, Loss: 0.05975513160228729\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.0523507334291935\n",
      "训练次数: 1800, Loss: 0.08566469699144363\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.05317181348800659\n",
      "训练次数: 2000, Loss: 0.09867484122514725\n",
      "训练次数: 2100, Loss: 0.09554747492074966\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.049454350024461746\n",
      "训练次数: 2300, Loss: 0.02695690654218197\n",
      "-----------------第11轮训练开始--------------------\n",
      "训练次数: 2400, Loss: 0.05198610946536064\n",
      "训练次数: 2500, Loss: 0.09425021708011627\n",
      "-----------------第12轮训练开始--------------------\n",
      "训练次数: 2600, Loss: 0.06994123011827469\n",
      "训练次数: 2700, Loss: 0.040835876017808914\n",
      "训练次数: 2800, Loss: 0.09943486750125885\n",
      "整体测试数据集上的Loss: 10.580761551856995\n",
      "整体测试数据集上的acc: 0.9277\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.042363185435533524\n",
      "训练次数: 200, Loss: 0.08400540053844452\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.04619262367486954\n",
      "训练次数: 400, Loss: 0.07606738805770874\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.04396788403391838\n",
      "训练次数: 600, Loss: 0.04659823700785637\n",
      "训练次数: 700, Loss: 0.13579469919204712\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.1122606098651886\n",
      "训练次数: 900, Loss: 0.046753957867622375\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.1033802255988121\n",
      "训练次数: 1100, Loss: 0.052729666233062744\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.09855734556913376\n",
      "训练次数: 1300, Loss: 0.06494014710187912\n",
      "训练次数: 1400, Loss: 0.07010623812675476\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.05827878788113594\n",
      "训练次数: 1600, Loss: 0.051671870052814484\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.029835201799869537\n",
      "训练次数: 1800, Loss: 0.05053986981511116\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.05674263462424278\n",
      "训练次数: 2000, Loss: 0.05963434651494026\n",
      "训练次数: 2100, Loss: 0.024630799889564514\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.05893844738602638\n",
      "训练次数: 2300, Loss: 0.07926653325557709\n",
      "-----------------第11轮训练开始--------------------\n",
      "训练次数: 2400, Loss: 0.04275873675942421\n",
      "训练次数: 2500, Loss: 0.07759323716163635\n",
      "-----------------第12轮训练开始--------------------\n",
      "训练次数: 2600, Loss: 0.05990525335073471\n",
      "训练次数: 2700, Loss: 0.04824991896748543\n",
      "训练次数: 2800, Loss: 0.021881388500332832\n",
      "-----------------第13轮训练开始--------------------\n",
      "训练次数: 2900, Loss: 0.09387952089309692\n",
      "训练次数: 3000, Loss: 0.06894038617610931\n",
      "整体测试数据集上的Loss: 11.761616989970207\n",
      "整体测试数据集上的acc: 0.9276\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.03240962699055672\n",
      "训练次数: 200, Loss: 0.04440850764513016\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.08289797604084015\n",
      "训练次数: 400, Loss: 0.04857349395751953\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.12717203795909882\n",
      "训练次数: 600, Loss: 0.06978926062583923\n",
      "训练次数: 700, Loss: 0.05143998563289642\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.04082624986767769\n",
      "训练次数: 900, Loss: 0.14768505096435547\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.06034432351589203\n",
      "训练次数: 1100, Loss: 0.034083571285009384\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.03760610148310661\n",
      "训练次数: 1300, Loss: 0.1010964959859848\n",
      "训练次数: 1400, Loss: 0.06289113312959671\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.08675559610128403\n",
      "训练次数: 1600, Loss: 0.07944833487272263\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.07149282097816467\n",
      "训练次数: 1800, Loss: 0.04022713750600815\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.10491932928562164\n",
      "训练次数: 2000, Loss: 0.042090658098459244\n",
      "训练次数: 2100, Loss: 0.09262163192033768\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.07148604840040207\n",
      "训练次数: 2300, Loss: 0.07501441240310669\n",
      "-----------------第11轮训练开始--------------------\n",
      "训练次数: 2400, Loss: 0.07125446200370789\n",
      "训练次数: 2500, Loss: 0.06546764820814133\n",
      "-----------------第12轮训练开始--------------------\n",
      "训练次数: 2600, Loss: 0.09604046493768692\n",
      "训练次数: 2700, Loss: 0.05876721441745758\n",
      "训练次数: 2800, Loss: 0.058350905776023865\n",
      "-----------------第13轮训练开始--------------------\n",
      "训练次数: 2900, Loss: 0.04666416719555855\n",
      "训练次数: 3000, Loss: 0.05088340491056442\n",
      "-----------------第14轮训练开始--------------------\n",
      "训练次数: 3100, Loss: 0.027706287801265717\n",
      "训练次数: 3200, Loss: 0.04786514490842819\n",
      "整体测试数据集上的Loss: 11.906686291098595\n",
      "整体测试数据集上的acc: 0.9298\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.035009611397981644\n",
      "训练次数: 200, Loss: 0.06968839466571808\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.08154173195362091\n",
      "训练次数: 400, Loss: 0.05160331353545189\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.039021942764520645\n",
      "训练次数: 600, Loss: 0.1034712940454483\n",
      "训练次数: 700, Loss: 0.034102022647857666\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.05091254785656929\n",
      "训练次数: 900, Loss: 0.02860119380056858\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.027180297300219536\n",
      "训练次数: 1100, Loss: 0.014530966989696026\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.03918218985199928\n",
      "训练次数: 1300, Loss: 0.04937761649489403\n",
      "训练次数: 1400, Loss: 0.1300656646490097\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.020526690408587456\n",
      "训练次数: 1600, Loss: 0.037924204021692276\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.019679877907037735\n",
      "训练次数: 1800, Loss: 0.06806215643882751\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.04117637127637863\n",
      "训练次数: 2000, Loss: 0.06218044087290764\n",
      "训练次数: 2100, Loss: 0.04621288552880287\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.044773802161216736\n",
      "训练次数: 2300, Loss: 0.04403335973620415\n",
      "-----------------第11轮训练开始--------------------\n",
      "训练次数: 2400, Loss: 0.055043742060661316\n",
      "训练次数: 2500, Loss: 0.07495982199907303\n",
      "-----------------第12轮训练开始--------------------\n",
      "训练次数: 2600, Loss: 0.028623200953006744\n",
      "训练次数: 2700, Loss: 0.05060824006795883\n",
      "训练次数: 2800, Loss: 0.02051571011543274\n",
      "-----------------第13轮训练开始--------------------\n",
      "训练次数: 2900, Loss: 0.09646926820278168\n",
      "训练次数: 3000, Loss: 0.02468477003276348\n",
      "-----------------第14轮训练开始--------------------\n",
      "训练次数: 3100, Loss: 0.05137696489691734\n",
      "训练次数: 3200, Loss: 0.035847872495651245\n",
      "-----------------第15轮训练开始--------------------\n",
      "训练次数: 3300, Loss: 0.02931300736963749\n",
      "训练次数: 3400, Loss: 0.03920750319957733\n",
      "训练次数: 3500, Loss: 0.05705661326646805\n",
      "整体测试数据集上的Loss: 12.328642891719937\n",
      "整体测试数据集上的acc: 0.929\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.06463121622800827\n",
      "训练次数: 200, Loss: 0.046626001596450806\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.042349692434072495\n",
      "训练次数: 400, Loss: 0.03923676162958145\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.025662491098046303\n",
      "训练次数: 600, Loss: 0.05317714810371399\n",
      "训练次数: 700, Loss: 0.030350878834724426\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.07091575860977173\n",
      "训练次数: 900, Loss: 0.09649559110403061\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.026629038155078888\n",
      "训练次数: 1100, Loss: 0.01847723498940468\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.07377086579799652\n",
      "训练次数: 1300, Loss: 0.07874570786952972\n",
      "训练次数: 1400, Loss: 0.03536580130457878\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.06733403354883194\n",
      "训练次数: 1600, Loss: 0.05248405039310455\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.05458945780992508\n",
      "训练次数: 1800, Loss: 0.024348145350813866\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.05735986679792404\n",
      "训练次数: 2000, Loss: 0.054263755679130554\n",
      "训练次数: 2100, Loss: 0.04342220351099968\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.037265948951244354\n",
      "训练次数: 2300, Loss: 0.02847333252429962\n",
      "-----------------第11轮训练开始--------------------\n",
      "训练次数: 2400, Loss: 0.018252767622470856\n",
      "训练次数: 2500, Loss: 0.056251004338264465\n",
      "-----------------第12轮训练开始--------------------\n",
      "训练次数: 2600, Loss: 0.04484393075108528\n",
      "训练次数: 2700, Loss: 0.06851544976234436\n",
      "训练次数: 2800, Loss: 0.033092521131038666\n",
      "-----------------第13轮训练开始--------------------\n",
      "训练次数: 2900, Loss: 0.03257114812731743\n",
      "训练次数: 3000, Loss: 0.05219682678580284\n",
      "-----------------第14轮训练开始--------------------\n",
      "训练次数: 3100, Loss: 0.058404695242643356\n",
      "训练次数: 3200, Loss: 0.031816765666007996\n",
      "-----------------第15轮训练开始--------------------\n",
      "训练次数: 3300, Loss: 0.03268885985016823\n",
      "训练次数: 3400, Loss: 0.05940086394548416\n",
      "训练次数: 3500, Loss: 0.050699394196271896\n",
      "-----------------第16轮训练开始--------------------\n",
      "训练次数: 3600, Loss: 0.06866994500160217\n",
      "训练次数: 3700, Loss: 0.02266923524439335\n",
      "整体测试数据集上的Loss: 12.658811815083027\n",
      "整体测试数据集上的acc: 0.929\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.060477856546640396\n",
      "训练次数: 200, Loss: 0.03940202668309212\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.03327437490224838\n",
      "训练次数: 400, Loss: 0.023778459057211876\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.021072033792734146\n",
      "训练次数: 600, Loss: 0.019652068614959717\n",
      "训练次数: 700, Loss: 0.034534260630607605\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.045193858444690704\n",
      "训练次数: 900, Loss: 0.03186767175793648\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.044469352811574936\n",
      "训练次数: 1100, Loss: 0.03266650065779686\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.0365656279027462\n",
      "训练次数: 1300, Loss: 0.020651891827583313\n",
      "训练次数: 1400, Loss: 0.019590355455875397\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.06728636473417282\n",
      "训练次数: 1600, Loss: 0.0468757301568985\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.02767261676490307\n",
      "训练次数: 1800, Loss: 0.063166044652462\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.02866811491549015\n",
      "训练次数: 2000, Loss: 0.04997504875063896\n",
      "训练次数: 2100, Loss: 0.036223404109478\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.0836133286356926\n",
      "训练次数: 2300, Loss: 0.03325992450118065\n",
      "-----------------第11轮训练开始--------------------\n",
      "训练次数: 2400, Loss: 0.04056910052895546\n",
      "训练次数: 2500, Loss: 0.0327620804309845\n",
      "-----------------第12轮训练开始--------------------\n",
      "训练次数: 2600, Loss: 0.02761857584118843\n",
      "训练次数: 2700, Loss: 0.03413147106766701\n",
      "训练次数: 2800, Loss: 0.06739962100982666\n",
      "-----------------第13轮训练开始--------------------\n",
      "训练次数: 2900, Loss: 0.03295042738318443\n",
      "训练次数: 3000, Loss: 0.09324789047241211\n",
      "-----------------第14轮训练开始--------------------\n",
      "训练次数: 3100, Loss: 0.025467075407505035\n",
      "训练次数: 3200, Loss: 0.010260685347020626\n",
      "-----------------第15轮训练开始--------------------\n",
      "训练次数: 3300, Loss: 0.01721658930182457\n",
      "训练次数: 3400, Loss: 0.06257086247205734\n",
      "训练次数: 3500, Loss: 0.09217222034931183\n",
      "-----------------第16轮训练开始--------------------\n",
      "训练次数: 3600, Loss: 0.05091724544763565\n",
      "训练次数: 3700, Loss: 0.04339166358113289\n",
      "-----------------第17轮训练开始--------------------\n",
      "训练次数: 3800, Loss: 0.01777627132833004\n",
      "训练次数: 3900, Loss: 0.043228570371866226\n",
      "整体测试数据集上的Loss: 12.971169494092464\n",
      "整体测试数据集上的acc: 0.9252\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.047855522483587265\n",
      "训练次数: 200, Loss: 0.05764518305659294\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.0296034999191761\n",
      "训练次数: 400, Loss: 0.04458410292863846\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.026286859065294266\n",
      "训练次数: 600, Loss: 0.031548045575618744\n",
      "训练次数: 700, Loss: 0.0776941254734993\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.022424643859267235\n",
      "训练次数: 900, Loss: 0.021743491291999817\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.036081213504076004\n",
      "训练次数: 1100, Loss: 0.040600959211587906\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.04422822967171669\n",
      "训练次数: 1300, Loss: 0.022261332720518112\n",
      "训练次数: 1400, Loss: 0.03746846690773964\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.03341794013977051\n",
      "训练次数: 1600, Loss: 0.04114970564842224\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.08771788328886032\n",
      "训练次数: 1800, Loss: 0.03594603389501572\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.019455885514616966\n",
      "训练次数: 2000, Loss: 0.06238030642271042\n",
      "训练次数: 2100, Loss: 0.04026394709944725\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.03072868287563324\n",
      "训练次数: 2300, Loss: 0.05064661428332329\n",
      "-----------------第11轮训练开始--------------------\n",
      "训练次数: 2400, Loss: 0.06325102597475052\n",
      "训练次数: 2500, Loss: 0.06936751306056976\n",
      "-----------------第12轮训练开始--------------------\n",
      "训练次数: 2600, Loss: 0.036705195903778076\n",
      "训练次数: 2700, Loss: 0.014620020985603333\n",
      "训练次数: 2800, Loss: 0.019114220514893532\n",
      "-----------------第13轮训练开始--------------------\n",
      "训练次数: 2900, Loss: 0.02507106587290764\n",
      "训练次数: 3000, Loss: 0.002743328455835581\n",
      "-----------------第14轮训练开始--------------------\n",
      "训练次数: 3100, Loss: 0.011306286789476871\n",
      "训练次数: 3200, Loss: 0.034686945378780365\n",
      "-----------------第15轮训练开始--------------------\n",
      "训练次数: 3300, Loss: 0.030902182683348656\n",
      "训练次数: 3400, Loss: 0.04377705603837967\n",
      "训练次数: 3500, Loss: 0.10295664519071579\n",
      "-----------------第16轮训练开始--------------------\n",
      "训练次数: 3600, Loss: 0.03468224033713341\n",
      "训练次数: 3700, Loss: 0.024180516600608826\n",
      "-----------------第17轮训练开始--------------------\n",
      "训练次数: 3800, Loss: 0.030242914333939552\n",
      "训练次数: 3900, Loss: 0.048088762909173965\n",
      "-----------------第18轮训练开始--------------------\n",
      "训练次数: 4000, Loss: 0.024936901405453682\n",
      "训练次数: 4100, Loss: 0.02320852130651474\n",
      "训练次数: 4200, Loss: 0.030382277444005013\n",
      "整体测试数据集上的Loss: 14.184095948934555\n",
      "整体测试数据集上的acc: 0.9256\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.04095500707626343\n",
      "训练次数: 200, Loss: 0.026891490444540977\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.018910063430666924\n",
      "训练次数: 400, Loss: 0.032404690980911255\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.030949745327234268\n",
      "训练次数: 600, Loss: 0.03649704530835152\n",
      "训练次数: 700, Loss: 0.034696538001298904\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.06427119672298431\n",
      "训练次数: 900, Loss: 0.038566865026950836\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.04359784722328186\n",
      "训练次数: 1100, Loss: 0.011713994666934013\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.039989665150642395\n",
      "训练次数: 1300, Loss: 0.041469745337963104\n",
      "训练次数: 1400, Loss: 0.027127329260110855\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.0782485380768776\n",
      "训练次数: 1600, Loss: 0.05245041847229004\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.02939445897936821\n",
      "训练次数: 1800, Loss: 0.036023810505867004\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.01746811904013157\n",
      "训练次数: 2000, Loss: 0.01987941935658455\n",
      "训练次数: 2100, Loss: 0.021025165915489197\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.07588927447795868\n",
      "训练次数: 2300, Loss: 0.05394306778907776\n",
      "-----------------第11轮训练开始--------------------\n",
      "训练次数: 2400, Loss: 0.017213594168424606\n",
      "训练次数: 2500, Loss: 0.014977062121033669\n",
      "-----------------第12轮训练开始--------------------\n",
      "训练次数: 2600, Loss: 0.05581866577267647\n",
      "训练次数: 2700, Loss: 0.013258432038128376\n",
      "训练次数: 2800, Loss: 0.017514653503894806\n",
      "-----------------第13轮训练开始--------------------\n",
      "训练次数: 2900, Loss: 0.03092055581510067\n",
      "训练次数: 3000, Loss: 0.055704496800899506\n",
      "-----------------第14轮训练开始--------------------\n",
      "训练次数: 3100, Loss: 0.017244281247258186\n",
      "训练次数: 3200, Loss: 0.03663147985935211\n",
      "-----------------第15轮训练开始--------------------\n",
      "训练次数: 3300, Loss: 0.026088226586580276\n",
      "训练次数: 3400, Loss: 0.015378442592918873\n",
      "训练次数: 3500, Loss: 0.06422799825668335\n",
      "-----------------第16轮训练开始--------------------\n",
      "训练次数: 3600, Loss: 0.02921678125858307\n",
      "训练次数: 3700, Loss: 0.02899016998708248\n",
      "-----------------第17轮训练开始--------------------\n",
      "训练次数: 3800, Loss: 0.010491634719073772\n",
      "训练次数: 3900, Loss: 0.045660339295864105\n",
      "-----------------第18轮训练开始--------------------\n",
      "训练次数: 4000, Loss: 0.06756819784641266\n",
      "训练次数: 4100, Loss: 0.020684566348791122\n",
      "训练次数: 4200, Loss: 0.07062216103076935\n",
      "-----------------第19轮训练开始--------------------\n",
      "训练次数: 4300, Loss: 0.06320521980524063\n",
      "训练次数: 4400, Loss: 0.036902960389852524\n",
      "整体测试数据集上的Loss: 14.542884845286608\n",
      "整体测试数据集上的acc: 0.9242\n",
      "-----------------第1轮训练开始--------------------\n",
      "训练次数: 100, Loss: 0.008375182747840881\n",
      "训练次数: 200, Loss: 0.05286578834056854\n",
      "-----------------第2轮训练开始--------------------\n",
      "训练次数: 300, Loss: 0.0228826142847538\n",
      "训练次数: 400, Loss: 0.051861975342035294\n",
      "-----------------第3轮训练开始--------------------\n",
      "训练次数: 500, Loss: 0.012468240223824978\n",
      "训练次数: 600, Loss: 0.023816844448447227\n",
      "训练次数: 700, Loss: 0.024858102202415466\n",
      "-----------------第4轮训练开始--------------------\n",
      "训练次数: 800, Loss: 0.018557801842689514\n",
      "训练次数: 900, Loss: 0.025680802762508392\n",
      "-----------------第5轮训练开始--------------------\n",
      "训练次数: 1000, Loss: 0.01722070202231407\n",
      "训练次数: 1100, Loss: 0.026400700211524963\n",
      "-----------------第6轮训练开始--------------------\n",
      "训练次数: 1200, Loss: 0.030592437833547592\n",
      "训练次数: 1300, Loss: 0.03815946727991104\n",
      "训练次数: 1400, Loss: 0.033906299620866776\n",
      "-----------------第7轮训练开始--------------------\n",
      "训练次数: 1500, Loss: 0.029164187610149384\n",
      "训练次数: 1600, Loss: 0.02617936208844185\n",
      "-----------------第8轮训练开始--------------------\n",
      "训练次数: 1700, Loss: 0.033236026763916016\n",
      "训练次数: 1800, Loss: 0.05643036589026451\n",
      "-----------------第9轮训练开始--------------------\n",
      "训练次数: 1900, Loss: 0.02604425884783268\n",
      "训练次数: 2000, Loss: 0.017741341143846512\n",
      "训练次数: 2100, Loss: 0.04624585807323456\n",
      "-----------------第10轮训练开始--------------------\n",
      "训练次数: 2200, Loss: 0.04335948824882507\n",
      "训练次数: 2300, Loss: 0.03960676118731499\n",
      "-----------------第11轮训练开始--------------------\n",
      "训练次数: 2400, Loss: 0.015820350497961044\n",
      "训练次数: 2500, Loss: 0.025334220379590988\n",
      "-----------------第12轮训练开始--------------------\n",
      "训练次数: 2600, Loss: 0.026989014819264412\n",
      "训练次数: 2700, Loss: 0.02651040256023407\n",
      "训练次数: 2800, Loss: 0.044685229659080505\n",
      "-----------------第13轮训练开始--------------------\n",
      "训练次数: 2900, Loss: 0.036542296409606934\n",
      "训练次数: 3000, Loss: 0.08273721486330032\n",
      "-----------------第14轮训练开始--------------------\n",
      "训练次数: 3100, Loss: 0.023665113374590874\n",
      "训练次数: 3200, Loss: 0.01953088492155075\n",
      "-----------------第15轮训练开始--------------------\n",
      "训练次数: 3300, Loss: 0.01094451081007719\n",
      "训练次数: 3400, Loss: 0.023375948891043663\n",
      "训练次数: 3500, Loss: 0.04967667534947395\n",
      "-----------------第16轮训练开始--------------------\n",
      "训练次数: 3600, Loss: 0.018726741895079613\n",
      "训练次数: 3700, Loss: 0.008002988062798977\n",
      "-----------------第17轮训练开始--------------------\n",
      "训练次数: 3800, Loss: 0.05645989254117012\n",
      "训练次数: 3900, Loss: 0.017181022092700005\n",
      "-----------------第18轮训练开始--------------------\n",
      "训练次数: 4000, Loss: 0.01882781647145748\n",
      "训练次数: 4100, Loss: 0.12092515826225281\n",
      "训练次数: 4200, Loss: 0.032742053270339966\n",
      "-----------------第19轮训练开始--------------------\n",
      "训练次数: 4300, Loss: 0.01692027784883976\n",
      "训练次数: 4400, Loss: 0.019265606999397278\n",
      "-----------------第20轮训练开始--------------------\n",
      "训练次数: 4500, Loss: 0.056556057184934616\n",
      "训练次数: 4600, Loss: 0.02057013474404812\n",
      "整体测试数据集上的Loss: 15.043552622199059\n",
      "整体测试数据集上的acc: 0.9284\n"
     ]
    }
   ],
   "source": [
    "# epoch 自己设定值，这里要求设定成一个列表，看看不同epoch对结果的影响\n",
    "for epoch in range(1, epochs+1):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "9bf3c482",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./FahionModel.pkl\"\n",
    "torch.save(model, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
